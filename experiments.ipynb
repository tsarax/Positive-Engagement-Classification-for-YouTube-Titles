{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "viqUqJf4IO8L",
        "colab_type": "code",
        "outputId": "6a0d0175-683d-463b-ebc1-9d13fe1bb385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import json  \n",
        "import pandas as pd  \n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import datetime\n",
        "import string\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "nltk.download('punkt')\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/Text Analytics Project')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRgimjkYYdcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_US = pd.read_csv('USvideos.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMVshtFlCwuw",
        "colab_type": "code",
        "outputId": "55d9dd80-0aeb-419e-85fa-ff2c78023f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "data_US.columns.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['video_id', 'trending_date', 'title', 'channel_title',\n",
              "       'category_id', 'publish_time', 'tags', 'views', 'likes',\n",
              "       'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled',\n",
              "       'ratings_disabled', 'video_error_or_removed', 'description'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAxPe0uzYdpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#views, likes, dislikes, comment_count\n",
        "data_US['ratio'] = data_US['likes']/(data_US['likes']+data_US['dislikes'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dAIQIYIiOSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_US.loc[data_US['ratio']>= 0.98, 'Rating_Category'] = 2\n",
        "data_US.loc[(data_US['ratio']< 0.98) & (data_US['ratio']>=0.94), 'Rating_Category'] = 1\n",
        "data_US.loc[data_US['ratio']< 0.94, 'Rating_Category'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLMVH9hYaYbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_data = data_US[['title', 'channel_title', 'views', 'likes', 'dislikes', 'comment_count', 'ratio', 'Rating_Category']]\n",
        "use_data = use_data.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjqngD9Hy1gc",
        "colab_type": "code",
        "outputId": "aea86b40-b8e7-4332-d5cd-0a58623e543d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(use_data[use_data['Rating_Category']==0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKeIYzEKaYgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Function to normalize text\n",
        "def text_norm(tokenized_sentence):\n",
        "    \"\"\" Takes in a tokenized text to normalize by removing puncuation, numbers, etc. and returns normalized tokens\"\"\"\n",
        "    corpus_normalized=[]\n",
        "    for document in tokenized_sentence: \n",
        "        normalized_sentences=[]\n",
        "        for i in document:\n",
        "            #get rid of punct, white space, numbers, etc.\n",
        "            sent = i.lower() #convert to lowercase\n",
        "            sent = re.sub(r'\\d+', '', sent) \n",
        "            sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "            sent = sent.strip() \n",
        "            sent = re.sub(r'(?:\\n|\\s+|\\t)', ' ', sent) \n",
        "            normalized_sentences.append(sent) \n",
        "        normalized_sentences = [i for i in normalized_sentences if i] # remove empty string tokens\n",
        "        corpus_normalized.append(normalized_sentences) #append sublists all to one list\n",
        "    return corpus_normalized  #list of lists\n",
        "\n",
        "def nltk_word_token(d):\n",
        "    \"\"\" Takes in corpus list and returns a list of words within each document (aka list of list). \"\"\"\n",
        "    tokenized_word = []\n",
        "    for document in d:\n",
        "        word = word_tokenize(document)\n",
        "        tokenized_word.append(word)\n",
        "    return tokenized_word\n",
        "\n",
        "def nltk_stem(tokenized_word):\n",
        "    \"\"\"Takes in corpus list and appends stemmed words to a list that is returned. \"\"\"\n",
        "    ps = PorterStemmer()\n",
        "    stem_lists=[]\n",
        "    for d in tokenized_word:\n",
        "        stemmed_words=[]\n",
        "        for word in d: \n",
        "            stemmed_words.append(ps.stem(word))\n",
        "        stem_lists.append(stemmed_words)\n",
        "    return stem_lists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7gpaarRdp_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_list = use_data['title'].tolist()\n",
        "rating_cat_list = use_data['Rating_Category'].tolist()\n",
        "\n",
        "\n",
        "\n",
        "# now map our tokenize function to each element of the list of texts\n",
        "tokenized_texts=nltk_word_token(title_list)\n",
        "normalized_tokens=text_norm(tokenized_texts)\n",
        "stemmed_tokens = nltk_stem(normalized_tokens)\n",
        "\n",
        "titles = []\n",
        "for title in stemmed_tokens:\n",
        "    titles.append(' '.join(title))\n",
        "df1 = pd.DataFrame({'Rating_Category': rating_cat_list, 'Title': titles})\n",
        "df1['Rating_Category'] = df1['Rating_Category'].astype(int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfKTM_Jwdp5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def try_SVM(df, ngram_range, loss_list, C_list):\n",
        "  with open('Proj_SVM_model.txt', \"w\") as f:\n",
        "\n",
        "    for i in ngram_range:\n",
        "      for j in loss_list:\n",
        "        for w in C_list: \n",
        "            tfidf = TfidfVectorizer(sublinear_tf=True, min_df=100, norm='l2', ngram_range=i, stop_words='english')\n",
        "            features = tfidf.fit_transform(df.Title.tolist())\n",
        "            saved = tfidf.fit(df.Title.tolist())\n",
        "            labels = df.Rating_Category\n",
        "            tfidf_name = f\"proj_tfidf_{i}_{j}_{w}.pkl\"\n",
        "            pickle.dump(saved, open(tfidf_name, 'wb'))\n",
        "\n",
        "\n",
        "            model = LinearSVC(loss=j, C=w, random_state=132)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.30, random_state=132)\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            class_report = metrics.classification_report(y_test, y_pred)\n",
        "\n",
        "            model_name = f\"proj_model_{i}_{j}_{w}.pkl\"\n",
        "            pickle.dump(model, open(model_name, 'wb'))\n",
        "\n",
        "\n",
        "            print(\"\\nFor {0} grams and {1} penalty and {2} regularization strength, the size of the feature matrix is:\".format(i, j, w), file=f)\n",
        "            print(features.shape, file=f)\n",
        "            print(class_report, file=f)\n",
        "            print(\"\\n\", file=f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYlkm1HGfSyn",
        "colab_type": "code",
        "outputId": "47cc7064-6344-4f7c-8ed7-40985363d218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# SVM MODEL\n",
        "import pickle\n",
        "ngram_range = [(1, 1), (1, 2)]\n",
        "C_list = [0.1, 1.0, 10]\n",
        "loss_list = ['hinge', 'squared_hinge']\n",
        "try_SVM(df1, ngram_range, loss_list, C_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX3LwbIqfSwK",
        "colab_type": "code",
        "outputId": "aaaba811-ead2-481a-b816-e9b1307ae582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, Dropout\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "import gensim\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import keras\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Lq-k1lfStx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model hyper parameters\n",
        "EMBEDDING_DIM = 100\n",
        "SEQUENCE_LENGTH_PERCENTILE = 90\n",
        "n_layers = 2\n",
        "hidden_units = 500\n",
        "batch_size = [50, 100, 200]\n",
        "pretrained_embedding = False\n",
        "# if we have pre-trained embeddings, specify if they are static or non-static embeddings\n",
        "TRAINABLE_EMBEDDINGS = True\n",
        "patience = [2, 4, 6]\n",
        "dropout_rate = 0.3\n",
        "n_filters = 100\n",
        "window_size = [8, 10]\n",
        "dense_activation = \"relu\"\n",
        "l2_penalty = [0.0003, 0.001, 0.01]\n",
        "epochs = 10\n",
        "VALIDATION_SPLIT = 0.1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ_W12indp23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_to_index(token, dictionary):\n",
        "    \"\"\"\n",
        "    Given a token and a gensim dictionary, return the token index\n",
        "    if in the dictionary, None otherwise.\n",
        "    Reserve index 0 for padding.\n",
        "    \"\"\"\n",
        "    if token not in dictionary.token2id:\n",
        "        return None\n",
        "    return dictionary.token2id[token] + 1\n",
        "\n",
        "def texts_to_indices(text, dictionary):\n",
        "    \"\"\"\n",
        "    Given a list of tokens (text) and a gensim dictionary, return a list\n",
        "    of token ids.\n",
        "    \"\"\"\n",
        "    result = list(map(lambda x: token_to_index(x, dictionary), text))\n",
        "    return list(filter(None, result))\n",
        "\n",
        "\n",
        "def train(train_texts, train_labels, dictionary, batch_size, window_size, l2_penalty, patience, model_file=None, EMBEDDINGS_MODEL_FILE=None):\n",
        "    \"\"\"\n",
        "    Train a word-level CNN text classifier.\n",
        "    :param train_texts: tokenized and normalized texts, a list of token lists, [['sentence', 'blah', 'blah'], ['sentence', '2'], .....]\n",
        "    :param train_labels: the label for each train text\n",
        "    :param dictionary: A gensim dictionary object for the training text tokens\n",
        "    :param model_file: An optional output location for the ML model file\n",
        "    :param EMBEDDINGS_MODEL_FILE: An optinal location for pre-trained word embeddings file location\n",
        "    :return: the produced keras model, the validation accuracy, and the size of the training examples\n",
        "    \"\"\"\n",
        "    assert len(train_texts)==len(train_labels)\n",
        "    # compute the max sequence length\n",
        "    # why do we need to do that?\n",
        "    lengths=list(map(lambda x: len(x), train_texts))\n",
        "    a = np.array(lengths)\n",
        "    MAX_SEQUENCE_LENGTH = int(np.percentile(a, SEQUENCE_LENGTH_PERCENTILE))\n",
        "    # convert all texts to dictionary indices\n",
        "    train_texts_indices = list(map(lambda x: texts_to_indices(x, dictionary), train_texts))\n",
        "    # pad or truncate the texts\n",
        "    x_data = pad_sequences(train_texts_indices, maxlen=int(MAX_SEQUENCE_LENGTH))\n",
        "    # convert the train labels to one-hot encoded vectors\n",
        "    train_labels = keras.utils.to_categorical(train_labels)\n",
        "    y_data = train_labels\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # create embeddings matrix from word2vec pre-trained embeddings, if provided\n",
        "    if pretrained_embedding:\n",
        "        embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDINGS_MODEL_FILE, binary=True)\n",
        "        embedding_matrix = np.zeros((len(dictionary) + 1, EMBEDDING_DIM))\n",
        "        for word, i in dictionary.token2id.items():\n",
        "            embedding_vector = embeddings_index[word] if word in embeddings_index else None\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        model.add(Embedding(len(dictionary) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=TRAINABLE_EMBEDDINGS))\n",
        "    else:\n",
        "        model.add(Embedding(len(dictionary) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH))\n",
        "    # add drop out for the input layer, why do you think this might help?\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    # add a 1 dimensional conv layer\n",
        "    # a rectified linear activation unit, returns input if input > 0 else 0\n",
        "    model.add(Conv1D(filters=n_filters,\n",
        "                     kernel_size=window_size,\n",
        "                     activation='relu'))\n",
        "    # add a max pooling layer\n",
        "    model.add(MaxPooling1D(MAX_SEQUENCE_LENGTH - window_size + 1))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # add 0 or more fully connected layers with drop out\n",
        "    for _ in range(n_layers):\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(hidden_units,\n",
        "                        activation=dense_activation,\n",
        "                        kernel_regularizer=l2(l2_penalty),\n",
        "                        bias_regularizer=l2(l2_penalty),\n",
        "                        kernel_initializer='glorot_uniform',\n",
        "                        bias_initializer='zeros'))\n",
        "\n",
        "    # add the last fully connected layer with softmax activation\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(len(train_labels[0]),\n",
        "                    activation='softmax',\n",
        "                    kernel_regularizer=l2(l2_penalty),\n",
        "                    bias_regularizer=l2(l2_penalty),\n",
        "                    kernel_initializer='glorot_uniform',\n",
        "                    bias_initializer='zeros'))\n",
        "\n",
        "    # compile the model, provide an optimizer\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # print a summary\n",
        "    print(model.summary())\n",
        "\n",
        "    # train the model with early stopping\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "    Y = np.array(y_data)\n",
        "\n",
        "    fit = model.fit(x_data,\n",
        "                    Y,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=VALIDATION_SPLIT,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "    print(fit.history.keys())\n",
        "    val_accuracy = fit.history['acc'][-1]\n",
        "\n",
        "    print(val_accuracy)\n",
        "    # save the model\n",
        "\n",
        "    if model_file:\n",
        "        model.save(model_file)\n",
        "    return model, val_accuracy, len(train_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQet7mKnaNFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydict = gensim.corpora.Dictionary(stemmed_tokens)\n",
        "\n",
        "\n",
        "for w in window_size:\n",
        "  for j in batch_size:\n",
        "    for p in patience:\n",
        "      for l in l2_penalty:\n",
        "        model_file = \"cnn_{0}_{1}_{2}_{3}.model\".format(w,j, p, l)\n",
        "        print(\"\\n\\n\\n\\n\\n\\n  The following is for window size of {0} and batch size of {1} and patience of {2} and l2 penalty of {3}\".format(w,j, p, l))\n",
        "        train(stemmed_tokens, rating_cat_list, mydict, batch_size=j, window_size=w, l2_penalty=l, patience=p, model_file=model_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xybuV00TaNCD",
        "colab_type": "code",
        "outputId": "54880f10-68a2-4ded-f836-7034809d9775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "mydict = gensim.corpora.Dictionary(stemmed_tokens)\n",
        "\n",
        "mydict.save('cnn.dict')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mwVD5raM-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfSNWU0yaM9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPJSX4CKaM6u",
        "colab_type": "code",
        "outputId": "37547c68-4b6b-492f-b800-c70432cfbee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x[0]==2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chgbCGogaM3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj3ukiiNaM1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiRu3p8haMyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LXQ9Ka3aMtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GioIQzdsNolW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}